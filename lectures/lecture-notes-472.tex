\documentclass[10pt]{article}
% Math Packages
\usepackage{float} % Required to use [H]
\usepackage{amsmath, mathtools}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{breqn}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{calc}
\usepackage{forest}
\usepackage{tikz-qtree}
\graphicspath{ {./images/} }
\usepackage{hyperref}
\usepackage[capitalize]{cleveref}
\usepackage[shortlabels]{enumitem}
\usetikzlibrary{arrows,matrix,positioning}
\usepackage{multicol}

\usepackage{mathtools}% http://ctan.org/pkg/mathtools
\usepackage{abraces} %xyz



\usepackage{empheq} % for boxed equations
\usepackage[most]{tcolorbox}
\newtcbox{\mymath}[1][]{%
  nobeforeafter, math upper, tcbox raise base,
  enhanced, colframe=blue!30!black,
  colback=blue!30, boxrule=1pt,
  #1}
\newtcbox{\boxmath}[1][]{%
  nobeforeafter, math upper, tcbox raise base,
  enhanced, colframe=blue!30!black,
  boxrule=1pt,
  #1}

% for the pipe symbol
\usepackage[T1]{fontenc}


% Citing theorems by name. (source: https://tex.stackexchange.com/questions/109843/cleveref-and-named-theorems)
\makeatletter
\newcommand{\ncref}[1]{\cref{#1}\mynameref{#1}{\csname r@#1\endcsname}}

\def\mynameref#1#2{%
  \begingroup
  \edef\@mytxt{#2}%
  \edef\@mytst{\expandafter\@thirdoffive\@mytxt}%
  \ifx\@mytst\empty\else
  \space(\nameref{#1})\fi
  \endgroup
}
\makeatother

% Colorful Notes
\usepackage{color} \definecolor{Red}{rgb}{1,0,0} \definecolor{Blue}{rgb}{0,0,1}
\definecolor{Purple}{rgb}{.5,0,.5} \def\red{\color{Red}} \def\blue{\color{Blue}}
\def\gray{\color{gray}} \def\purple{\color{Purple}}
\newcommand{\rnote}[1]{{\red [#1]}} % \rnote{foo} gives '[foo]' in red
\newcommand{\pnote}[1]{{\purple [#1]}} % \pnote{foo} gives '[foo]' in purple
\newcommand{\bnote}[1]{{\blue #1}} % \bnote{foo} gives 'foo' in blue
\newcommand{\gnote}[1]{{\gray #1}} % \gnote{foo} gives 'foo' in gray
\newcommand{\Max}[1]{{\purple [#1]}} % \bnote{foo} then 'foo' is blue


% Claim numbering (the counter restarts after each proof environment)
\newcounter{claimcount}
\setcounter{claimcount}{0}
\newenvironment{claim}{\refstepcounter{claimcount}\par\addvspace{\medskipamount}\noindent\textbf{Claim \arabic{claimcount}:}}{}
\usepackage{etoolbox}
\AtBeginEnvironment{proof}{\setcounter{claimcount}{0}}
\newenvironment{claimproof}{\par\addvspace{\medskipamount}\noindent\textit{Proof of Claim  \arabic{claimcount}.}}{\hfill\ensuremath{\qedsymbol} \tiny{Claim}

  \medskip}
% Add claim support to cleverref
\crefname{claimcount}{Claim}{Claims}


% Math Environments
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{question}[theorem]{Question}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}


\usepackage{skull}

% Redefine the Example environment to include "End of example [number]"
\makeatletter
\let\oldexample\example
\renewenvironment{example}
{\begin{oldexample}}
  {\par\smallskip\hfill   End of Example~\theexample. $\square$    \par\end{oldexample}}
\makeatother


% Matrices and Column Vectors. 
\usepackage{stackengine}
\setstackgap{L}{1.0\normalbaselineskip}
\usepackage{tabstackengine}
\setstackEOL{;}% row separator
\setstackTAB{,}% column separator
\setstacktabbedgap{1ex}% inter-column gap
\setstackgap{L}{1.5\normalbaselineskip}% inter-row baselineskip
\let\nmatrix\bracketMatrixstack  %Usage: \nmatrix{1,2,3\4,5,6}
\newcommand\cv[1]{\setstackEOL{,}\bracketMatrixstack{#1}} %usage: \cv{1,2,3}

% Custom Math Coqmmands
\newcommand{\vt}{\vskip 5mm} % vertical space
\newcommand{\fl}{\noindent\textbf} % first line
\newcommand{\Fl}{\vt\noindent\textbf} % first line with space above
\newcommand{\norm}[1]{\left\lVert#1\right\rVert} % norm
\newcommand{\pnorm}[1]{\left\lVert#1\right\rVert_p} % p-norm
\newcommand{\qnorm}[1]{\left\lVert#1\right\rVert_q} % q-norm
\newcommand{\1}[1]{\textbf{1}_{\left[#1\right]}} % indicator function 
\def\limn{\lim_{n\to\infty}} % shortcut for lim as n-> infinity
\def\sumn{\sum_{n=1}^{\infty}} % shortcut for sum from n=1 to infinity
\def\sumkn{\sum_{k=1}^{n}} % shortcut for sum from k=1 to n
\def\sumin{\sum_{i=1}^{n}} % shortcut for sum from i=1 to n
\def\SAs{\sigma\text{-algebras}} % shortcut for $\sigma$-algebras
\def\SA{\sigma\text{-algebra}} % shortcut for $\sigma$-algebra
\def\Ft{\mathcal{F}_t} % time-indexed sigma-algebra (t)
\def\Fs{\mathcal{F}_s} % time-indexed sigma-algebra (s)
\def\F{\mathcal{F}} % sigma-algebra
\def\G{\mathcal{G}} % sigma-algebra
\def\R{\mathbb{R}} % Real numbers
\def\N{\mathbb{N}} % Natural numbers
\def\Z{\mathbb{Z}} % Integers
\def\E{\mathbb{E}} % Expectation
\def\P{\mathbb{P}} % Probability
\def\Q{\mathbb{Q}} % Q probability
\def\dist{\text{dist}} %Text 'dist' for things like 'dist(x,y)'
\newcommand{\indep}{\perp \!\!\! \perp}  %independence symbol
\def\Var{\mathrm{Var}} % Variance
\def\tr{\mathrm{tr}} % trace

% Brackets and Parentheses
\def\[{\left [}
    \def\]{\right ]}
% \def\({\left (}
%   \def\){\right )}




\usepackage{color}
\definecolor{Red}{rgb}{1,0,0}
\definecolor{Blue}{rgb}{0,0,1}
\definecolor{Purple}{rgb}{.5,0,.5}
\def\red{\color{Red}}
\def\blue{\color{Blue}}
\def\gray{\color{gray}}
\def\purple{\color{Purple}}
\definecolor{RoyalBlue}{cmyk}{1, 0.50, 0, 0}
\newcommand{\dempfcolor}[1]{{\color{RoyalBlue}#1}} 
\newcommand{\demph}[1]{\textcolor{RoyalBlue}{\textbf{\slshape #1}}} % Slanted
% RoyalBlue text

% comment exactly one of the following line to show / hide the solutions
% \newcommand{\solution}[1]{{\purple #1}} % uncomment to show the solutions
\newcommand{\solution}[1]{} % uncomment to hide the solutions



\title{Lecture Notes for Math 472: \\Statistical Inference}
\date{Last updated: \today}
% \author{mh}

\begin{document}
\maketitle




\tableofcontents
\newpage
\setcounter{section}{-1}
\section{Tentative course outline}

This course is a problem-oriented introduction to the basic concepts of probability and statistics,
providing a foundation for applications and further study.
\begin{enumerate}
  \item \textbf{Weeks 1-2.} Sampling distributions (4 lessons).

  chi-squared, t, and F distributions, distributions of sample mean and
  variance

  \item \textbf{Weeks 3-4.} Point estimation (5 lessons)

  properties and methods of point estimation

  \item \textbf{Weeks 5-6.} Interval estimation (4 lessons)

  Confidence intervals for means, variances, proportions and differences

  \item \textbf{Weeks 7-12.} Hypothesis Testing (19 lessons)

  Neyman-Pearson lemma, likelihood ratio test; tests concerning means and
  variances, tests based on count data, nonparametric tests, analysis of variance

  \item \textbf{Weeks 13-14.} Regression and correlation (6 lessons)

  regression, bivariate normal distributions, method of least squares
\end{enumerate}



\newpage
\section{2025-01-12 | Week 01 |  Lecture 01}

\begin{itemize}
  \item give syllabus
  \item do activity with why you're in this course
\end{itemize}


\begin{center}
  \begin{tcolorbox}[width=0.9\textwidth, colback=white, colframe=black]
    \textit{\textbf{The nexus question of this lecture:} What is a
      probability?}
  \end{tcolorbox}
\end{center}
\textbf{Reading assignment:} Sections 1.1, 1.2, 1.3, 2.1, 2.4 of the textbook.

\subsection{What is probability?}
\subsubsection{A general framework: sample space, events, etc}
We begin with a general framework and some terminology to formalize the
notions of probability. This is based on section 2.4 in the textbook.

\begin{itemize}
  \item An \demph{experiment} is an activity or process whose outcome is
  subject to uncertainty, and about which an observation is made.

  Examples include flipping a coin, rolling a dice, measuring the size of a
  wave, or the amount of rainfall, conducting a poll, performing a diagnostic
  test, opening a pack of Pokemon cards, etc.

  \item The \demph{sample space} $S$ of an experiment is the set of all
  possible outcomes. The elements of the sample space are called \demph{sample
    points}.

  We think of each sample point as representing a unique outcome of the
  experiment. In the case of rolling a dice, the sample points are $1,2,3,4,5$
  and $6$, and the sample space is $S=\left\{1,2,3,4,5,6\right\}$.


  \item We use the term \demph{event} to refer to a collection of
  outcomes, i.e., a subset of $S$.

  Example: if our experiment is rolling a 6-sided dice, here are some events

  \begin{equation*}
    \begin{aligned}
      A &= [\text{observe an odd number}]\\
      B &= [\text{observe an even number}]\\
      C &= [\text{observe a number less than 5}]\\
      D &= [\text{observe a 2 or a 3}]\\
      E_1 &= [\text{observe a 1}]
    \end{aligned}
    \qquad
    \begin{aligned}
      E_2 &= [\text{observe a 2}]\\
      E_3 &= [\text{observe a 3}]\\
      E_4 &= [\text{observe a 4}]\\
      E_5 &= [\text{observe a 5}]\\
      E_6 &= [\text{observe a 6}]
    \end{aligned}
  \end{equation*}

  \item There are two types of events: \demph{compound events}, which can be
  decomposed into other events, and \demph{simple events}, which cannot.

  In the above example, the events $A,B,C$ and $D$ are compound events. $E_{1},\ldots,E_{6}$ are simple events.

  \item A sample space is \demph{discrete} if it is countable (i.e., finite or
  countably infinite). In a discrete sample space $S$, the set of all possible
  events is the \textit{power set} of $S$.\footnote{If $S$ is not discrete, a
    complication arises: in that case, some subsets of $S$ are too wild and
    untameable for us to treat them mathematically as ``events''. Resolving
    that issue requires introducing measure theory, which is beyond the scope
    of this class, so we will ignore it and simply steer clear of any setting
    where any issues might arise.}

  In the dice-rolling example, the set of all possible events is
  $\left\{E: E\subseteq \left\{1,2,3,4,5,6\right\} \right\}$. 
  
  \begin{equation*}
    \begin{aligned}
      A &= [\text{observe an odd number}] = \{1,3,5\} \\
      B &= [\text{observe an even number}] = \{2,4,6\} \\
      C &= [\text{observe a number less than 5}] = \{1,2,3,4\} \\
      D &= [\text{observe a 2 or a 3}] = \{2,3\}\\
      E_1 &= [\text{observe a 1}] = \{1\}
    \end{aligned}
    \qquad
    \begin{aligned}
      E_2 &= [\text{observe a 2}] = \{2\} \\
      E_3 &= [\text{observe a 3}] = \{3\} \\
      E_4 &= [\text{observe a 4}] = \{4\} \\
      E_5 &= [\text{observe a 5}] = \{5\} \\
      E_6 &= [\text{observe a 6}] = \{6\}
    \end{aligned}
  \end{equation*}

  \item Some observations about events:
  \begin{itemize}
    \item The sample points are \textit{elements} of $S$. The simple events are
    \textit{singleton subsets} of $S$. In the dice example, we have:
    \begin{itemize}
      \item Sample points: 1,2,3,4,5,6.
      \item Simple events: $\left\{1\right\},\left\{2\right\},\left\{3\right\},\left\{4\right\},\left\{5\right\},\left\{6\right\}$.
    \end{itemize}
    \item The empty set $\emptyset$ and the whole sample space $S$ are always
    both events: $\emptyset$ is the event ``nothing happens'' and $S$ is the
    event ``something happens''.
    \item Events satisfy the properties of a boolean algebra:
    \begin{itemize}
      \item \textbf{``And'':} If $E$ and $F$ are events, then $E\cap F$ is the
      event that $E$ and $F$ occur.
      \item \textbf{``Or'':} If $E$ and $F$ are events, then $E\cup F$ is the
      event that $E$ or $F$ occurs.
      \item \textbf{``Not'':} If $E$ is an event, then $E^{c}= S\backslash E$
      is the event that $E$ does not occur.
    \end{itemize}
  \end{itemize}
  \item Two events $E$ and $F$ are \demph{mutually exclusive} if
  $E\cap F =\emptyset$. This means that $E$ and $F$ cannot both happen at the
  same time.

  In the dice example, the events $A$ and $B$ are mutually exclusive, since
  the dice roll cannot be both even and odd. But $A$ and $C$ are not mutually
  exclusive because $A\cap C =\left\{1,3\right\}\neq \emptyset$. If a 1 or a 3
  is rolled, then both $A$ and $C$ occur.
\end{itemize}

\subsubsection{Definition of probability measure}
\begin{definition}[Probability measure]
  Let $S$ be a sample space associated with an experiment. A function $\P$ is
  said to be a \demph{probability measure} on $S$ if it satisfies the
  following three axioms:
  \begin{enumerate}[label=\textbf{A.\arabic*}]
    \item (Nonnegativity) \label{item:probability-axiom-nonnegativity} For
    every event $E\subseteq S$,
    \begin{equation*}
      \P\left[E\right] \geq 0.
    \end{equation*}
    \item (Total mass one) $\P\left[S \right] =1$.
    \item (Countable additivity) If $E_{1},E_{2},\ldots$ is a sequence of
    events which are pairwise mutually exclusive (meaning
    $E_{i}\cap E_{j}=\emptyset$ if $i\neq j$), then
    \begin{equation*}
      \P\left[E_{1}\cup E_{2}\cup \ldots \right] = \sum_{i=1}^{\infty} \P\left[E_{i} \right].
    \end{equation*}
  \end{enumerate}
  If $\P$ is a probability measure, then for every event $E\subseteq S$, the
  number $\P\left[E \right]$ is called the \demph{probability} of $E$.
\end{definition}

The above definition only tells us the conditions an assignment of
probabilities must satisfy; it doesn't tell us how to assign specific
propabilities to events.

Probability measures satisfy some basic properties:

\begin{proposition}[Basic properties of probability measure]
  \label{prop:some-deductions-from-axioms}
  If $\P$ is a probability measure, then the following properties hold:
  \begin{enumerate}[label=\rm{(\roman*.)}]
    \item (The null event has probability zero) \label{item:empty-set-has-prob-zero}
    $\P\left[\emptyset \right] = 0$.
    \item (Finite additivity) \label{item:finite-additivity} Let $\left\{E_{1},\ldots,E_{n}\right\}$ be a
    \textit{finite} sequence of events. If the sequence is pairwise disjoint, then
    \begin{equation*}
      \P\left[E_{1}\cup E_{2}\cup\ldots\cup E_{n} \right] 
      = \P\left[E_{1} \right]+\P\left[E_{2} \right]+\ldots+\P\left[E_{n} \right].
    \end{equation*}
    \item (``With probability one, an event $E$ either does occur or doesn't'') \label{item:complements} $\P\left[E^{c} \right] = 1-\P\left[E \right]$.
    \item (Excision Property) \label{item:excision-property} If $A,B$ are events and $A\subseteq B$, then
    \begin{equation*}
      \P\left[B\backslash A \right] = \P\left[B \right] - \P\left[A \right].
    \end{equation*}
    \item (``The particular is less likely than the general'') \label{item:subsets} If $A,B$ are events and $A\subseteq B$, then
    $\P\left[ A\right] \leq \P\left[B \right]$.
    \item (``Probabilities are between 0 and 1'') \label{item:probabilities-are-in-unit-interval} For any event $E$,  $\P\left[E \right]\in [0,1]$.
  \end{enumerate}
\end{proposition}

\section{2026-01-14 | Week 01 | Lecture 02}
\begin{center}
  \begin{tcolorbox}[width=0.9\textwidth, colback=white, colframe=black]
    \textit{\textbf{The topic of this lecture:} independent events, conditional
      probabilities, random variables}
  \end{tcolorbox}
\end{center}

\subsection{Independent events and conditional probabilities}
\textit{This section is based on section 2.7 in the textbook.}

\begin{definition}[Independence]
  \label{def:independence}
  Two events $A$ and $B$ are said to be \demph{independent} if $\P\left[A\cap
    B \right] = \P\left[A \right] \P\left[B \right]$. Otherwise, the events
  are said to be dependent.
\end{definition}

\begin{definition}[Conditional probability]
  Let $A,B$ be events, and assume that $\P\left[B\right]>0$. Then the
  \demph{conditional probability of $A$, given $B$}, denoted
  $\P\left[A\mid B \right]$, is given by the formula
  \begin{equation*}
    \P\left[A\mid B \right] = \frac{\P\left[A\cap B \right]}{\P\left[B \right]}.
  \end{equation*}
\end{definition}

\fl{Interpretation:} $\P\left[A\mid B \right]$ is the probability of $A$ when
we know that event $B$ happened.

\begin{definition}
  \label{def:positive-relationship}
  We say that there exists a \demph{positive relationship} between events $A$
  and $B$ if
  \begin{equation*}
    \P\left[A\mid B \right]> \P\left[A \right],
  \end{equation*}
  and a \demph{negative relationship} if
  \begin{equation*}
    \P\left[A\mid B \right]< \P\left[A \right].
  \end{equation*}
\end{definition}

\begin{remark}
  Note the the conditions of \cref{def:positive-relationship} are symmetric in
  the sense that
  \begin{equation*}
    \P\left[A\mid B \right]> \P\left[A \right] \iff \P\left[B\mid A \right]>
    \P\left[B \right],
  \end{equation*}
  provided that both $A$ and $B$ have positive probability.
\end{remark}

\begin{example}
  Roll a 6-sided dice. Let $A$ be the event that a `2' was rolled, and $B$ be
  the event that an even number was observed.

  \begin{itemize}
    \item The unconditional probability: $\P\left[A \right]=1/6$.
    \item The conditional probability: $\P\left[A\mid B \right] = 1/3$.
  \end{itemize}
  Since $\frac{1}{3}>\frac{1}{6}$, we conclude there is a positive
  relationship between rolling a `2' and rolling an even number.
\end{example}

The notion of independence formalizes the idea of ``no relationship''.

\begin{proposition}
  \label{prop:independence-interpretation}
  If $A,B$ are events with positive probabilities then the following are
  equivalent:
  \begin{enumerate}[label=\rm{(\roman*.)}]
    \item $A$ and $B$ are independent.
    \item $\P\left[A\mid B \right] = \P\left[A \right]$
    and $\P\left[B\mid A \right] = \P\left[B \right]$.
  \end{enumerate}
\end{proposition}
In words, independence means that the probabilities of each event are
unaffected by whether or not the other event occurs.
\cref{prop:independence-interpretation} simply formalizes this idea using
conditional probabilities.


\subsection{Random variables}
\textit{Based on Sections 2.11, 4.2 in the textbook}


\begin{definition}[Random variable]
  A \demph{random variable} (or \demph{rv}) is a real-valued function whose domain is a sample
  space.
\end{definition}

The value of a random variable is thought of as varying depending on the
outcome of the experiment (the sample point). Random variables are usually
denoted with capital letters, like $X,Y,Z$.

\begin{example}[Sum 2d4]
  \label{ex:sum-2d4}
  Roll a 4-sided dice twice (this is the \textbf{experiment}). There are 16 possible
  \textbf{outcomes}. The \textbf{sample space} is
  \begin{equation*}
    S = \left\{(x,y): x,y\in \left\{1,2,3,4\right\} \right\}.
  \end{equation*}
  Let $X$ be the sum of the two rolls. We can represent $X$ by the following table:
  \begin{equation*}
    \begin{array}{c@{\quad}c|cccc} & \multicolumn{5}{c}{\text{Dice 2}} \\ & &
      1 & 2 & 3 & 4 \\ \cline{2-6}  & 1 & 2 & 3 & 4 & 5 \\ \text{Dice 1} & 2 & 3 & 4 & 5 & 6 \\ & 3 & 4 & 5 & 6 & 7 \\ & 4 & 5 & 6 & 7 & 8 \end{array}
  \end{equation*}
  \textbf{Events} are often defined using preimages of random variables. Most
  interesting take the form $[X\in B]$, where $X$ is a random variable and
  $B\subseteq \R$. For example, the event that $X=6$ is:
  \begin{align*}
    [X=6] 
    &= \left\{\omega\in S: X(\omega) = 6\right\} \\
    &= \left\{(1,5), (2,4), (3,3), (4,2), (5,1)\right\}.
  \end{align*}
  The textbook uses the notation $\left\{X=6\right\}$ instead of
  $\left[ X=6 \right]$.

  Here's another example of an event. Let $E = \left\{2,4,6,8\right\}$. Then
  \begin{align*}
    \left[ X \text{ is even} \right]
    &= \left[ X \in E \right]\\
    &= \left\{\omega\in S: X(\omega) \in E \right\} \\
    &= \left\{(1,1), (1,3), (2,2), (2,4), (3,1), (3,3), (4,2), (4,4) \right\}.
  \end{align*}

  When writing random variables, we usually suppress the arguments, e.g.,
  writing $X$ rather than $X(\omega)$.

  
\end{example}
% \begin{example}
%   Consider an experiment in which we flip a coin twice. The sample space is
%   consists of four sample points:
%   \begin{equation*}
%     E_{1} = HH, \quad E_{2}= HT, \quad E_{3} = TH, \quad E_{4} = TT
%   \end{equation*}
%   Let $Y$ be the number of heads flipped. So $Y$ can take three values: $0,1$
%   and $2$. These are examples of events:
%   \begin{equation*}
%     \left[ Y=0 \right] = \left\{E_{4}\right\},
%     \quad \left[ Y=1 \right] = \left\{E_{2},E_{3}\right\},
%     \quad \text{and} \quad  \left[ Y=2 \right] = \left\{E_{1}\right\}.    
%   \end{equation*}
%   We can compute the probabilities of these:
%   \begin{equation*}
%     \P\left[Y=0 \right] = \P\left[E_{4} \right] = 1/4.
%   \end{equation*}
%   and
%   \begin{equation*}
%     \P\left[Y=1 \right] = \P\left[E_{2} \right]+ \P\left[E_{3} \right] = 1/4+1/4 = 1/2.
%   \end{equation*}
% \end{example}

% \begin{definition}[Support]
%   Let $X$ be a random variable. The \demph{support} of $X$, denoted
%   $\text{supp}(X)$ or $S_{X}$, is the smallest closed set $F\subseteq \R$ such
%   that $\P\left[X\in F \right]=1$.
% \end{definition}
% Informally, the support of a random variable is the set of all possible values
% that it can take. In \cref{ex:sum-2d4},
% $\text{supp}(X)= \left\{2,3,4,5,6,7,8\right\}$.
\newpage

\section{2026-01-16 | Week 01 | Lecture 03}

\subsection{Random variables}
\subsubsection{Discrete vs continuous}
\begin{definition}[Discrete random variable]
  \label{def:discrete-random-variable}
  We say that a random variable $X$ is a \demph{discrete random variable} if
  it can assume only a finite or countably infinite number of distinct values.
\end{definition}


\begin{definition}[Probability mass function, pmf]
  Let $X$ be a discrete random variable. The \demph{probability mass function}
  (or \demph{pmf}) of $X$ is the function
  \begin{equation*}
    p(x) = \P\left[X=x \right],
  \end{equation*}
  defined for every $x\in \R$.
\end{definition}

\begin{example}
  The pmf of $X$ in \cref{ex:sum-2d4} is
  \begin{equation*}
    p(2) = 1/16 \quad p(3) = 2/16, \quad p(4) = 3/16, \quad p(5) = 4/16, \quad p(6) = 3/16, \quad p(7)=2/16, \quad p(8) = 1/16
  \end{equation*}
  and $p(x)=0$ for all other $x\in \R$.
\end{example}


\begin{definition}[Distribution function - section 4.2]
  Let $X$ be any random variable. The \demph{cumulative distribution function}
  (or \demph{cdf}) of $X$ is the function
  \begin{equation*}
    F(x) = \P\left[X \leq x\right],
  \end{equation*}
  defined for all $x\in \R$.
\end{definition}

\begin{remark}
  The domain of a cdf is always $\R$, and it is always a nondecreasing
  function with $F(-\infty)=0$ and $F(+\infty)=1$. The cdf of a discrete
  random variable is always a step function.
\end{remark}

\begin{definition}[Continuous rv]
  \label{def:continuous-rv}
  Let $Y$ be a random variable with distribution function $F$. We say that $Y$
  is a \demph{continuous random variable} if there exists a nonnegative
  function $f$ such that
  \begin{equation}\label{eq:1}
    F(y) = \int_{-\infty}^{y}f(t)dt
  \end{equation}
  for all $y\in \R$. The function $f$ is called the \demph{probability density
    function} (or \demph{pdf}) of $Y$.
\end{definition}


\begin{remark}
  For continuous random variables, the distribution function $F$ is always
  continuous. Moreover, for a continuous random variable $Y$,
  $\P\left[Y=b \right]=0$ for all $b\in \R$.
  % \begin{equation*}
  %   \P\left[Y=b \right] 
  %   = \P\left[Y \leq b\right] - \lim_{a \to b^{-}}\P\left[Y \leq a\right] 
  %   = F(b) - \lim_{a \to b^{-}}F(a) = 0
  % \end{equation*}
  % where the last equality follows from continuity of $F$.
\end{remark}

\begin{theorem}[Theorem 4.3 in textbook]
  If $Y$ is a continuous random variable with pdf $f$, then
  \begin{equation*}
    \P\left[a \leq Y \leq b\right] = \int_{a}^{b}f(t)dt
  \end{equation*}
  for all $-\infty \leq a \leq b\leq +\infty$.
\end{theorem}
% \begin{proof}
%   \begin{equation*}
%     \P\left[a \leq Y \leq b\right] = \P\left[Y=a \right]+ \P\left[a<Y \leq b\right]
%   \end{equation*}
%   The result then follows from the following two computations:
%   \begin{align*}
%     \P\left[Y=a \right] = F(a) - F(a-) = \lim_{\epsilon \to 0} \int_{a-\epsilon}^{a}f(t)dt = 0,
%   \end{align*}
%   and
%   \begin{align*}
%     \P\left[a < Y \leq  b\right]
%     &= F(b) - F(a)\\
%     &= \int_{-\infty}^{b}f(t)dt - \int_{-\infty}^{a}f(t)dt\\ 
%     &= \int_{a}^{b}f(t)dt. 
%   \end{align*}
% \end{proof}

\subsubsection{Expected value}
\begin{definition}[Expectation of a continuous random variable]
  \label{def:expectation-continuous-random-variable}
  If $Y$ is a random variable with pdf $f$, then the \demph{expected value} of
  $Y$, denoted $\E\left[Y \right]$, is the quantity
  \begin{equation*}
    \E\left[ Y\right] = \int_{-\infty}^{\infty} y f(y)dy,
  \end{equation*}
  provided that $\int_{-\infty}^{\infty} |y|f(y)dy<\infty$. 
\end{definition}

\begin{remark}
  $\E\left[Y \right]$ is the long-run average of $Y$, if we were to repeat the
  experiment many times.
\end{remark}

The next theorem is called the \textit{Law of the unconscious statistician (LOTUS)}.


\begin{theorem}[LOTUS - single variable case]
  \label{thm:law-unconscious-statistician}
  Let $g:\R \to \R$ be a function.
  \begin{enumerate}[label=\rm{(\roman*.)}]
    \item If $X$ has pmf $p$, then
    \begin{equation*}
      \E\left[g(X) \right]= \sum_{\substack{x\in \R\\ p(x)>0}} g(x)p(x).
    \end{equation*}
    \item If $Y$ has pdf $f$, then
    \begin{equation*}
      \E\left[g(Y) \right] = \int_{-\infty}^{\infty}g(y)f(y)dy.
    \end{equation*}

  \end{enumerate}
\end{theorem}

\begin{remark}
  \label{rmk:need-for-joint-distn}
  Often we wish to compute probabilities of functions of multiple random
  variables, for example:
  \begin{itemize}
    \item What is the probability that
    $\frac{X_{1}+\ldots+X_{n}}{n}\in (0,1)$? Here, the function is
    $g(x_{1},\ldots,x_{n})= \frac{x_{1}+\ldots+x_{n}}{n}$.
    \item What is the probability that $\max(X,Y) \leq 10$? Here the function is
    $g(x,y)=\max(x,y).$
    \item Suppose we roll two dice and take the maximum. What is the expected
    value? In this case, our dice rolls are $X,Y$ and we want to compute
    $\E\left[g(X,Y) \right]$, where $g(x,y) = \max(x,y)$.
  \end{itemize}
  To answer these sorts of questions, we need the notion of a ``joint distribution''.
\end{remark}

\subsubsection{Joint distributions}
\textit{This subsection is based on section 5.4 in the textbook. Everything in
  this section generalizes naturally to $n$ variables, but the results are
  simpler to state for just 2 random variables.}



\begin{definition}[Joint pmf]
  \label{def:joint-pmf}
  Let $X_{1}$ and $X_{2}$ be discrete random variables. The \demph{joint
    probability mass function} for $X_{1}$ and $X_{2}$ is the function
  \begin{equation*}
    p(x_{1},x_{2}) = \P\left[X_{1}=x_{1},X_{2}=x_{2} \right],
  \end{equation*}
  defined for all $x_{1},x_{2}\in \R$.
\end{definition}

\begin{definition}[Joint pdf]
  \label{def:joint-pdf}
  Let $Y_{1}$ and $Y_{2}$ be continuous random variables. We say that $Y_{1}$ and $Y_{2}$
  are \demph{jointly continuous} if there exists a function
  $f:\R^{2} \to \R_{ \geq 0}$ such that 
  \begin{equation*}
    \P\left[Y_{1} \leq y_{1}, Y_{2} \leq  y_{2}\right] =  \int_{-\infty}^{y_{1}}\int_{-\infty}^{y_{2}}    f(t_{1},t_{2}) dt_{2}dt_{1}.
  \end{equation*}
  for all $y_{1},y_{2}\in \R$. The function $f$ is called the \demph{joint probability
  density function} for $Y_{1}$ and $Y_{2}$.
\end{definition}


\begin{theorem}[LOTUS - multivariable case]
  \label{thm:lotus-multivariable-case}
  Let $g:\R^{2}\to \R$.
  \begin{itemize}
    \item   If $X_{1},X_{2}$ have joint pmf $p(x_{1},x_{2})$, then
    \begin{equation*}
      \E\left[g(X_{1},X_{2}) \right] = \sum_{\substack{(x_{1},x_{2})\in \R^{2}:\\ p(x_{1},x_{2})>0}} g(x_{1},x_{2})p(x_{1},x_{2}).
    \end{equation*}
    \item If $Y_{1},Y_{2}$ are jointly continuous random variables with joint
    pdf $f(y_{1},y_{2})$, then
    \begin{equation*}
      \E\left[g(Y_{1},Y_{2}) \right] = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g(y_{1},y_{2})f(y_{1},y_{2})dy_{1}dy_{2}.  
    \end{equation*}
  \end{itemize}
\end{theorem}

\begin{remark}
  \cref{thm:lotus-multivariable-case} generalizes to $n$ variables. It gives
  us a way to answer questions like the third question posed in
  \cref{rmk:need-for-joint-distn}.
\end{remark}

\newpage
\section{2026-01-21 | Week 02 | Lecture 04}

\begin{center}
  \begin{tcolorbox}[width=0.9\textwidth, colback=white, colframe=black]
    \textit{\textbf{Question for this lecture:} what does it mean for random
      variables to be independent, and what does it buy us?}
  \end{tcolorbox}
\end{center}


\subsection{Independence of random variables}

\subsubsection{Definition and characterization}
The aim of this section is to define what it means for random variables to be
independent.

\begin{definition}
  \label{def:independence-random-variables}
  We say that the random variables $X_{1},X_{2},\ldots,X_{n}$ are
  \demph{independent} if the following holds for all possible values
  $x_{1},\ldots,x_{n}$ in their range:
  \begin{equation*}
    \P\left[X_{1}\leq x_{1},X_{2}\leq x_{2},\ldots,X_{n}\leq x_{n}\right] = \P\left[X_{1}\leq x_{1}\right] \P\left[X_{2}\leq x_{2}\right] \cdots \P\left[X_{n}\leq x_{n}\right].
  \end{equation*}
\end{definition}

\begin{theorem}[Factorization theorem -- Theorem 5.4 in textbook]
  \label{thm:factorization-theorem}
  For discrete/continuous random variables, independence is equivalent to
  factorizability of the joint pmf/pdf. More formally, we have:
  
  \begin{itemize}
    \item \textbf{Discrete case:} Let $X_{1},\ldots,X_{n}$ be discrete random
    variables. Then $X_{1},\ldots,X_{n}$ are independent if and only if
    \begin{equation*}
      \P\left[X_{1}=x_{1},X_{2}=x_{2},\ldots, X_{n}=x_{n} \right] = \P\left[X_{1}= x_{1}\right]\P\left[X_{2}= x_{2}\right]\cdots \P\left[X_{n}= x_{n}\right]
    \end{equation*}
    for all $x_{1},\ldots,x_{n}\in \R$.
    \item \textbf{Continuous case:} Let $Y_{1},\ldots,Y_{n}$ be continuous
    random variables with pdfs $f_{1},\ldots,f_{n}$, and joint pdf
    $f:\R^{n} \to \R$. Then $Y_{1},\ldots,Y_{n}$ are independent if and only if
    \begin{equation*}
      f(y_{1},\ldots,y_{n}) = f_{1}(y_{1}) f_{2}(y_{2})\cdots f_{n}(y_{n})
    \end{equation*}
    for all $y_{1},\ldots,y_{n}\in \R$.
  \end{itemize}
\end{theorem}

\begin{remark}
  In this course, we will frequently work with ``samples'' of $n$ independent
  random variables $X_{1},\ldots,X_{n}$. The intuition about independence for
  $n$ variables is that observing any number of them doesn't give you any
  information about the others.
\end{remark}
% \begin{definition}[Random vector]
%   Let $S$ be a sample space. A function\footnote{measureable} $X:S \to \R^{n}$
%   is called a \demph{random vector}.
% \end{definition}


% In other words, it is a vectors whose entries are random variables
% $X=(X_{1},\ldots,X_{n})$. The distribution of a random vector is simply the
% joint distribution of $X_{1},\ldots,X_{n}$.


\subsubsection{Independence and expectations/variances}

Independence splits the expectation of a product into a product of expectations.
\begin{theorem}
  \label{thm:independence-splits-expectations}
  If $X_{1},X_{2},\ldots X_{n},$ are independent, then
  \begin{equation*}
    \E\left[X_{1}X_{2}\cdots X_{n} \right] = \E\left[X_{1} \right] \E\left[X_{2} \right] \cdots \E\left[X_{n} \right],
  \end{equation*}
  provided that the expectations exist.
\end{theorem}
\begin{proof}
  This can be proven for continuous/discrete random variables by an
  application of \cref{thm:lotus-multivariable-case} and
  \cref{thm:factorization-theorem}.
\end{proof}


\begin{definition}[Variance of a random variable]
  \label{def:variance-random-variable}
  Let $X$ be a random variable. The \demph{variance} of $X$, denoted
  $\Var(X)$, is the quantity
  \begin{equation*}
    \Var(X) = \E\left[(X-\mu)^{2} \right].
  \end{equation*}
  where $\mu= \E\left[X \right]$. The positive square root of the variance is
  the \demph{standard deviation} of $X$.
\end{definition}

\begin{remark}
  Variance is often denoted by $\sigma^{2}$. The textbook uses the notation
  $V(X)$ instead of $\Var(X)$.
\end{remark}


\begin{theorem}[Properties of Variance]
  \label{thm:variance-properties}
  Let $X$ be a random variable and $a,b$ be scalars. Then
  \begin{equation*}
    \Var(aX+b) = a^{2}\Var(X).
  \end{equation*}
  If $X$ and $Y$ are independent random variables then
 \begin{equation*}
    \Var(X+Y) = \Var(X)+\Var(Y).
  \end{equation*}
\end{theorem}
\begin{proof}
  Follows by direct computation using the definition of variance.
\end{proof}

The second part of \cref{thm:variance-properties} says that for independent
random variables, variance is additive.

\subsubsection{Independence and long-term behavior: the weak law of large numbers}

Combining many independent sources of randomness tends to produce predictible
phenomena. Here we introduce one example of that.

\begin{theorem}[Weak Law of Large Numbers]
  \label{thm:weak-law-large-numbers}
  Let $(X_{n})_{n=1}^{\infty}$ be a sequence of independent random variables.
  Assume that for all $n$, $\mu= \E\left[X_{n} \right]$ and
  $\Var(X_{n}) \leq B$ for some fixed bound $B<\infty$. Let
  $S_{n}=X_{1}+\ldots+X_{n}$. Then for all $\epsilon>0$,
  \begin{equation*}
    \P\left[ \left| \frac{S_{n}}{n}-\mu \right| >\epsilon\right] \to 0 \text{
      as } n \to \infty.
  \end{equation*}
\end{theorem}

\newpage
\section{2026-01-23 | Week 02 | Lecture 05}
\subsection{Proof of the weak law of large numbers}

We will need the following theorem, which says that if the average male height
is 5 feet tall, then no more than 10\% of men can be more than 50 feet tall.

\begin{theorem}[Markov's inequality]
  \label{thm:markovs-inequality}
  Let $X$ be a nonnegative random variable and let $a>0$. Then
  \begin{equation}\label{eq:2}
    \P\left[X \geq a\right] \leq \frac{\E\left[X \right]}{a}
  \end{equation}
\end{theorem}
\begin{proof}
  Let $A= \left[ X \geq a\right]$. Then $X \cdot \mathbf{1}_{A}\leq X$, so
  \begin{align*}
    \E\left[X \right] 
    &\geq \E\left[X \cdot \mathbf{1}_{A} \right]\\
    &\geq \E\left[a \mathbf{1}_{A}\right]\\
    &\geq a \E\left[\mathbf{1}_{A} \right]\\
    &=a \P\left[A \right]\\
    &=a \P\left[X \geq a\right].
  \end{align*}
  Dividing by $a$ implies \cref{eq:2}.
\end{proof}


\begin{theorem}[Weak Law of Large Numbers]
  \label{thm:weak-law-large-numbers}
  Let $(X_{n})_{n=1}^{\infty}$ be a sequence of independent random variables.
  Assume that for all $n$, $\mu= \E\left[X_{n} \right]$ and
  $\Var(X_{n}) \leq B$ for some fixed bound $B<\infty$. Let
  $S_{n}=X_{1}+\ldots+X_{n}$. Then for all $\epsilon>0$,
  \begin{equation*}
    \P\left[ \left| \frac{S_{n}}{n}-\mu \right| >\epsilon\right] \to 0 \text{
      as } n \to \infty.
  \end{equation*}
\end{theorem}

\begin{proof}
  First observe that by independence,
  \begin{equation}\label{eq:3}
    \Var(S_{n}) = \Var(X_{1})+\ldots+\Var(X_{n}) \leq nB.
  \end{equation}
  Next, let $\epsilon>0$ be arbitrary.
  \begin{align*}
    \P\left[\left| \frac{S_{n}}{n}-\mu \right| >\epsilon\right]
    &= \P\left[\left| S_{n}-n\mu \right|> n\epsilon \right]\\
    &= \P\left[ \left( S_{n}-n\mu \right)^{2} > (n\epsilon)^{2}  \right]\\
    &\leq  \frac{\E\left[\left( S_{n}-n\mu \right)^{2}
      \right]}{n^{2}\epsilon^{2}} &&\text{by \cref{thm:markovs-inequality}}\\
    &= \frac{\Var(S_{n})}{n^{2}\epsilon^{2}} &&\text{by definition of variance }\\
    &\leq \frac{B}{\epsilon^{2}n} &&\text{by \cref{eq:3}}.
  \end{align*}
  The right hand side tends to zero as $n \to \infty$, proving the theorem.
\end{proof}

\subsection{Equality in distribution}
\begin{definition}[Identically distributed]
  \label{def:identically-distributed}
  Two random variables $X$ and $Y$ are \demph{identically distributed} if they
  have the same cdf.
\end{definition}
\begin{remark}
  Identically distributed random variables are said to ``have the same
  distribution''. For discrete/continuous random variables, this is equivalent
  to them having the same pmf/pdf.
\end{remark}

The next example shows that identically distributed random variables need not
be equal as functions.

\begin{example}
  Flip a coin. Define two random variables
  \begin{equation*}
    X = \left\{ \begin{array}{l@{\quad:\quad}l} 1 &\text{coin is heads} \\ 0&
        \text{coin is tails} \end{array}\right.
  \end{equation*}
  \begin{equation*}
    Y = \left\{ \begin{array}{l@{\quad:\quad}l} 0 &\text{coin is heads} \\ 1&
        \text{coin is tails} \end{array}\right.
  \end{equation*}
  Then $X$ and $Y$ are identically distributed, since they have the same pmf:
  \begin{equation*}
    \P\left[X=1 \right] = \P\left[Y=1 \right]=\frac{1}{2}
    \quad \text{and} \quad
    \P\left[X=0 \right] = \P\left[Y=0 \right]=\frac{1}{2}.
  \end{equation*}
  But of course $X$ and $Y$ are never equal, i.e., $\P\left[X\neq Y \right]=1$.
\end{example}

% \begin{theorem}[Law of Large Numbers]
%   \label{thm:law-large-numbers}
%   Let $X_{1},X_{2},\ldots$ be a sequence of independent random variables with
%   finite variance and common mean $\mu$. Then
%   \begin{equation*}
%     \frac{X_{1}+\ldots+X_{n}}{n} \to \mu
%   \end{equation*}
% \end{theorem}

\subsection{Populations and samples}


Here we introduce a conceptual framework for mathematical statistics.

A \demph{population} is a large body of data that is the target of our
interest. The subset collected from it is our \demph{sample}. 

\begin{example}[Populations]
  A population can be real or theoretical. Here are some examples
  \begin{itemize}
    \item The set of people in Hawaii (real, finite)
    \item The set of voters in the 2026 Midterm elections (hypothetical, finite)
    \item The decimal expansion of $\pi$ (countably infinite)
    \item The infinitely many observations that could be made during a
    laboratory experiment if the experiment were repeated over and over again
    (hypothetical)
    \item The lifetimes of light bulbs produced by a factory
  \end{itemize}
  Importantly, a population can also be a \textit{probability distribution},
  specified by a pdf, pmf, or cdf
  \begin{itemize}
    \item Observations made from an exponential distribution with mean
    $\lambda>0$ (i.e., the distribution with pdf
    $f(x)=\lambda e^{-\lambda x}\mathbf{1}_{\left[ x>0 \right] }$.)
  \end{itemize}
\end{example}



\newpage
\section{2026-01-26 | Week 03 | Lecture 06}
\subsection{Sampling}
\textit{For discussions of ``random sample'', see sections 2.12 and 6.1 in the textbook.}

The simpleset sampling procedure is called simple random sampling.

\begin{definition}[Simple random sampling]
  Let $N$ and $n$ denote the numbers of elements in the population and sample,
  respectively. If the sampling is conducted in such a way that each of the
  ${N\choose n}$ samples has an equal probability of being selected, the
  sampling is called \demph{simple random sampling}, and the result is a
  \demph{simple random sample}.
\end{definition}

More commonly in mathematical statistics, we think of the population as a
distribution.

\begin{definition}[Random sample from a distribution]
  Consider a given probability distribution on $\R$ that can be represented by
  a pdf or pmf $f$. We say that the random variables $X_{1},\ldots,X_{n}$ form
  a \demph{random sample} from this distribution if these random variables are
  independent and the distribution of each is given by $f$. Such random
  variables are also said to be \demph{independent and identically
    distributed} (iid). The number of random variables $n$ is the
  \demph{sample size}.
\end{definition}


\textit{The objective of statistics to is to make an inference about a
  population based on information contained in a sample from that population
  and to provide an associated measure of goodness for the inference.}


\subsection{What is a statistic?}
\textit{Section 7.1 in the textbook.}

\begin{definition}[Statistic]
  A \demph{statistic} is a function of the observable random variables in a
  sample and known constants.
\end{definition}

In other words, if $X_{1},\ldots,X_{n}$ is a random sample and
$T:\R^{n}\to \R$ is a function, then the random
variable
\begin{equation*}
  Y = T(X_{1},\ldots,X_{n})
\end{equation*}
is a statistic.\footnote{A statistic does not need to be real-valued, but we
  will focus on the case when it is.} The probability distribution of such a
statistic $Y$ is called its \demph{sampling distribution}.

Often, we have some quantity of interest, called a \demph{target parameter},
and we want a single ``best guess'' of some quantity of interest. When this is
the case, the we call a statistic an estimator:

\begin{definition}[Estimator]
  \label{def:estimator}
  An \demph{estimator} is a statistic, that is a function
  $T(X_{1},\ldots,X_{n})$ of a sample, that is used to approximate a target
  parameter.
  An \demph{estimate} is the realized value of an
  estimator (e.g., a number) that is obtained when the sample is actually
  taken.
\end{definition}
In words, an estimator is a rule, often expressed as a formula, that tells us
how to calculate the value of an estimate based on the measurements contained
in a sample. Here are two important examples of estimators.

\begin{definition}[Sample Mean, Sample Variance]
  Let $X_{1},\ldots,X_{n}$ be a random sample. The \demph{sample mean},
  denoted $\overline{X}$, is the statistic
  \begin{equation*}
    \overline{X} = \frac{1}{n}\sum_{i=1}^{n}X_{i}.
  \end{equation*}
  The \demph{sample variance}, denoted $S^{2}$, is the statistic
  \begin{equation*}
    S^{2} = \frac{1}{n-1} \sum_{i=1}^{n} (X_{i}-\overline{X})^{2}.
  \end{equation*}
\end{definition}

The target parameters these are used to estimate are the mean and variance of
the population.


The following examples illustrates some of the terminology we've introduced.

\begin{example}[Estimating average lightbulb lifetime]
  \label{ex:estimating-average-lightbulb-lifetime}
  Suppose the lifetime of each light bulb produced in a certain factory is
  distributed according to the following pdf
  \begin{equation*}
    f(y)= \lambda e^{-\lambda y}\mathbf{1}_{\left[ y>0 \right] }.
  \end{equation*}
  for some parameter $\lambda>0$, where $y$ is measured in minutes. Here, the
  \textbf{population} is either the set of lightbulbs produced by the factory,
  which we idealize as the probability distribution given by $f$. Suppose we
  do not know $\lambda$, and we wish to estimate it. Hence $\lambda$ is the
  \textbf{target parameter}. A random sample $Y_{1},\ldots,Y_{n}$ is taken.
  (So the $Y_{i}$'s are independent and each as pdf $f$.)

  In this case $f$ is an exponential distribution with rate $\lambda>0$. The
  exponential distribution is used to modeling lifetimes of systems that are
  not subject to degredation (e.g., lifetimes of light bulbs, not diesel
  engines) as well as waiting times (e.g., the length of time intervals
  between cars passing by a tree on an idyllic country road). In the road
  example, if the rate of cars passing is $2$ cars per minute, then on average
  you have to wait only $1/2$ of a minute for a car to pass. That is, the
  \textit{rate} and \textit{expected values} are reciprocals of each other.
  Let's prove this fact in the following claim:

  \begin{claim}
    If $Y$ has pdf $f$, then $\E\left[Y \right]= \frac{1}{\lambda}$.
  \end{claim}
  \begin{claimproof}
    By definition of expected value for a continuous random variable,
    \begin{align*}
      \E\left[Y \right]
      &= \int_{-\infty}^{\infty} y\lambda e^{-\lambda y} \mathbf{1}_{\left[ y>0 \right] }dy\\
      &= \int_{0}^{\infty} y\lambda e^{-\lambda y}dy\\ 
      &= \int_{0}^{\infty}y \frac{d}{dy} \left[ -e^{-\lambda y} \right] dy\\  
      &=\left[ -ye^{-\lambda y} \right]_{y=0}^{y=\infty}- \int_{0}^{\infty} \frac{d}{dy} \left[ y \right] \left( - e^{-\lambda y} \right) dy &&\text{by integration by parts}\\ 
      &=\int_{0}^{\infty}e^{-\lambda y}dy\\ 
      &= \left[ \frac{1}{\lambda}e^{-\lambda y} \right]^{y=0}_{y=\infty} \\
      &= \frac{1}{\lambda}.
    \end{align*}
  \end{claimproof}
\end{example}

\newpage
\section{2026-01-28 | Week 03 | Lecture 07}

\subsection{Basic examples of statistics and estimators}
\textit{This lecture is based on section 7.1 in the textbook.}

\begin{example}[\cref{ex:estimating-average-lightbulb-lifetime} continued]

  We have a random sample of $Y_{1},\ldots,Y_{n}$ from the distribution with
  pdf 
  \begin{equation*}
    f(y)= \lambda e^{-\lambda y}\mathbf{1}_{\left[ y>0 \right] },
  \end{equation*}
  where $\lambda$ is a target parameter.

  In the setting of this example, we interpret the random variables
  $Y_{1},\ldots,Y_{n}$ as lifetimes (in, say, years) of lightbulbs produced
  by a certain factory.
  
  In the last lecture, we used integration by parts to show that if $Y$ has
  pdf $f$, then
  \begin{equation*}
    \E\left[ Y \right]= \frac{1}{\lambda}.
  \end{equation*}

  Consider the \textbf{statistic}
  \begin{equation*}
    \overline{Y}= \frac{Y_{1}+\ldots+Y_{n}}{n}.
  \end{equation*} 
  By \cref{thm:weak-law-large-numbers} (The Weak Law of Large Numbers),
  $\overline{Y}$ is likely to be close to $\E\left[Y \right] = 1/\lambda$ when
  $n$ is large. Therefore $\frac{1}{\overline{Y}}$ is likely to be close to
  $\lambda$. Therefore a reasonable \textbf{estimator} for $\lambda$ is
  \begin{equation*}
    \frac{1}{\overline{Y}} = \frac{n}{Y_{1}+\ldots+Y_{n}}.
  \end{equation*}
  If $n=10$ and the observed values of $Y_{1},\ldots,Y_{10}$ are
  \begin{equation*}
    2,~ 0.8,~ 0.1,~ 2.3,~ 3.2,~ 5.4,~ 2,~ 0.4,~ 2.8,~ \text{ and }3.4
  \end{equation*}
  then $\overline{Y}=2.24$, and hence our \textbf{estimate} for $\lambda$ would be
  $\frac{1}{2.24}\approx 0.45$.
  
  % Here is one more useful calcuation.
  
  % \begin{claim}
  %   The \textbf{joint pdf} of $Y_{1},\ldots,Y_{n}$ is the function
  %   \begin{equation*}
  %     g(y_{1},\ldots,y_{n}) = \lambda^{n} e^{-\lambda(y_{1}+\ldots+y_{n})} \mathbf{1}_{\left[ \min(y_{1},\ldots,y_{n})>0 \right]},
  %   \end{equation*}
  %   defined for all $y_{1},\ldots,y_{n}\in \R$
  % \end{claim}
  % \begin{claimproof}
  %   Using the fact that $Y_{1},\ldots,Y_{n}$ in independent continuous random
  %   variables,
  %   \begin{align*}
  %     g(y_{1},\ldots,y_{n}) 
  %     &= f(y_{1})\cdots f(y_{n}) &&\text{by \cref{thm:factorization-theorem}}\\
  %     &= \prod_{i=1}^{n}\left(\lambda e^{-\lambda y_{i}}\mathbf{1}_{\left[ y_{i}>0 \right] }\right)\\ 
  %     &= \lambda^{n} e^{-\lambda(y_{1}+\ldots+y_{n})} \left( \prod_{i=1}^{n}\mathbf{1}_{\left[ y_{i}>0 \right] }  \right)\\ 
  %     &= \lambda^{n} e^{-\lambda(y_{1}+\ldots+y_{n})} \mathbf{1}_{\left[ \min(y_{1},\ldots,y_{n})>0 \right]}.
  %   \end{align*}
  % \end{claimproof} 

  % \begin{remark}
  %   In Claim 2, we computed the joint distribution of the sample. This is
  %   \textit{not} the sampling distribution (which is much harder to compute in
  %   this example).
  % \end{remark}
 
\end{example}


\begin{example}[Example 7.1 in textbook]
  Roll a dice three times. Let $Y_{1},Y_{2},Y_{3}$ be the values of the rolls.
  The average number observed in this sample of size 3 is
  \begin{equation*}
    \overline{Y} = \frac{Y_{1}+Y_{2}+Y_{3}}{3}.
  \end{equation*}
  This is the \textbf{sample mean}, it is function of the sample (and the known
  constant $n$) and is therefore a \textbf{statistic}.

  \Fl{Question:} What the mean $\mu_{\overline{Y}}$ and standard deviation
  $\sigma_{\overline{Y}}$ of the random variable $\overline{Y}$?

  First we compute the mean and variance of a single $Y_{i}$.
  \setcounter{claimcount}{0}
  \begin{claim}
    \label{claim:mean-and-variance-of-dice}
    $\E\left[Y_{i} \right]=3.5$ and $\Var(Y_{i}) = 35/12 \approx 2.9167$. 
  \end{claim}
  \begin{claimproof}
    Fix $i\in [3]$. For simplicity, let $Y = Y_{i}$. Using the definition of
    expected value,
    \begin{equation*}
      \E\left[Y \right] = \sum_{k=1}^{6}k \P\left[Y=k \right]=
      \sum_{k=1}^{6}k \frac{1}{6}=3.5.
    \end{equation*}
    Next we compute the variance of $Y$:
    \begin{align*}
      \Var(Y) 
      &= \E\left[\left( Y-3.5 \right)^{2}  \right] &&\text{\cref{def:variance-random-variable} }\\
      &= \sum_{y=1}^{6}(y-3.5)^{2} \P\left[Y=y \right] &&\text{By \cref{thm:law-unconscious-statistician} (LOTUS) using
                                                              $g(y)=(y-3.5)^{2}$ }\\
      &= \sum_{k=1}^{6}(y-3.5)^{2} \frac{1}{6} &&\text{since $\P\left[Y=y
                                                  \right]=1/6$ for all $k\in [6]$ }\\
      &= 35/12.
    \end{align*}
  \end{claimproof}

  
  \begin{claim}
    $\mu_{\overline{Y}}=\E\left[\overline{Y} \right] = 3.5$
  \end{claim}
  \begin{claimproof}
    We have:
    \begin{align*}
      \E\left[\overline{Y} \right] 
      &= \E\left[\frac{Y_{1}+Y_{2}+Y_{3}}{3} \right] &&\text{by definition of $\overline{Y}$}\\
      &= \frac{1}{3} \left( \E\left[Y_{1} \right]+\E\left[Y_{2}
        \right]+\E\left[Y_{3} \right] \right) &&\text{by linearity of expectation}\\ 
      &= \frac{1}{3} \left( 3.5+3.5+3.5 \right) &&\text{by \cref{claim:mean-and-variance-of-dice}}\\ 
      &= 3.5.
    \end{align*}
  \end{claimproof}


  \begin{claim}
    \label{claim:variance-of-average-of-3-dice-rolls}
    $\sigma^{2}_{\overline{Y}}=\Var(\overline{Y})= .9722$
  \end{claim}
  \begin{claimproof}
    \begin{align*}
      \Var(\overline{Y})
      &= \Var \left(  \frac{Y_{1}+Y_{2}+Y_{3}}{3} \right)\\ 
      &= \frac{1}{9}\Var \left( Y_{1}+Y_{2}+Y_{3} \right) &&\text{by \cref{thm:variance-properties} }\\ 
      &= \frac{1}{9}\left( \Var(Y_{1})+\Var(Y_{2})+\Var(Y_{3}) \right)
      &&\text{by \cref{thm:variance-properties}}\\
      &= \frac{1}{9} \left(2.9167+2.9167+2.9167 \right) &&\text{by \cref{claim:mean-and-variance-of-dice}}\\
      &= \frac{2.9167}{3} &&(*)
    \end{align*}
    and this is approximately $.97224$.
  \end{claimproof}

  Since the standard deviation is the square root of the variance, the
  standard deviation is
  \begin{equation*}
    \sigma_{\overline{Y}}=\sqrt{.9722}= .9860
  \end{equation*}
   \cref{claim:variance-of-average-of-3-dice-rolls}.

  \begin{remark}
    In equation $(*)$, we had $\Var(\overline{Y}) = \frac{2.9167}{3}$. Notice
    how, if we computed the variance of four or five dice rolls, we'd get
    \begin{equation*}
      \Var(\overline{Y})=\frac{2.9167}{4} \quad \text{or} \quad \Var(\overline{Y})=\frac{2.9167}{5}.
    \end{equation*}
    In general, for $n$ dice rolls, we'd get
    \begin{equation*}
      \Var(\overline{Y}) = \frac{2.9167}{n}.
    \end{equation*}
    This quantity tends to zero as $n \to \infty$. In words, as the number of
    samples $n$ grows, the variance of the sample mean decreases.
  \end{remark}
\end{example}

\newpage
\section{2026-01-30 | Week 03 | Lecture 08}

\subsection{The Gaussian distribution}
\textit{Begin section 7.2}

\begin{definition}[Normal / Gaussian Distribution]
  \label{def:gaussian-distribution}
  A random variable $Y$ is said to be \demph{normally distributed} with mean
  $\mu\in \R$ and variance $\sigma^{2}>0$ if it has pdf
  \begin{equation*}
    f(y) = \frac{1}{\sqrt{2\pi\sigma^{2}}} \exp\left\{-\frac{(y-\mu)^{2}}{2\sigma^{2}} \right\},
  \end{equation*}
  defined for all $y\in \R$. We abbreviate this by writing
  $Y\sim \mathcal{N}(\mu,\sigma^{2})$. If $Y\sim \mathcal{N}(0,1)$, we say $Y$
  has \demph{standard normal distribution}.
\end{definition}

\begin{remark}
  Normal random variables/distributions are also called \demph{Gaussian} random variables/distributions.
\end{remark}

\begin{example}
  Normal distributions are the classic ``bell curve'' distributions. Whenever
  a random quantity is the result of adding up many small, independent random
  effects, we expect it to be approximately normally distributed.

  \begin{itemize}
    \item A classic example is human height, which is influenced by the
    combined effects of thousands of genes together with environmental
    factors.
    \item Another example is repeated measurements of the same physical
    quantity (e.g., if we all went out and measured the temperature outside
    tomorrow at midday), this is because the total error is the sum of many
    small independent sources of noise (e.g., instrument precision, time,
    location, shade, wind, etc).
  \end{itemize}
\end{example}


\begin{remark}[Standardization]
  \label{rmk:standardization}
  If $Y \sim \mathcal{N}(\mu,\sigma^{2})$ then the random
  variable
  \begin{equation*}
    Z = \frac{X-\mu}{\sigma}
  \end{equation*}
  is a standard normal random variable. The transformation
  \begin{equation*}
    X \mapsto \frac{X-\mu}{\sigma}
  \end{equation*}
  in which we subtract off the mean of $X$ and then divide by its standard
  deviation is called \textbf{standardization}.
\end{remark}

\begin{remark}[The Empirical Rule]
  The \textbf{Empirical Rule} (also called the \textbf{68-95-99.7 rule}; see
  textbook section 1.3) is summarized by the following figure, for normal
  distributions:
  \begin{center}
    \includegraphics[scale=.25]{images/empirical-rule-normdist2.jpg}
    
    \textit{\tiny Image source: \url{https://andymath.com/wp-content/uploads/2019/12/empirical-rule-normdist2.jpg}}
  \end{center}
\end{remark}

The next theorem says that if we have a sample of $Y_{1},\ldots,Y_{n}$ normal
random variables, then their sample mean is normally distributed as well.

\begin{theorem}[Mean and variance of a normal sample mean]
  \label{thm:mean-and-variance-of-normal-sample-mean}
  Let $Y_{1},\ldots,Y_{n}$ be a random sample of size $n$ from a normal
  distribution with mean $\mu$ and variance $\sigma^{2}$. Then
  \begin{equation*}
    \overline{Y}= \frac{1}{n} \sum_{i=1}^{n}Y_{i} 
  \end{equation*}
  is normally distributed with mean $\mu_{\overline{Y}}=\mu$ and variance
  $\sigma^{2}_{\overline{Y}}=\sigma^{2}/n$.
\end{theorem}
\begin{proof}[Proof sketch]
  The proof follows from three facts:
  \begin{itemize}
    \item Any linear combination of normal random variables is a normal random
    variable (this can be proved using moment generating functions).
    \item That $\E\left[\overline{Y} \right]=\mu$ follows by linearity of
    expectation.
    \item The variance calculation follows by \cref{thm:variance-properties}.
  \end{itemize}
\end{proof}

\begin{remark}
  \label{rmk:sample-mean-standardized}
  Applying the idea of standardization, observe that the random variable
  $Z = \frac{\overline{Y}-\mu_{\overline{Y}}}{\sigma_{\overline{Y}}} =
  \frac{\overline{Y}-\mu}{\sigma/\sqrt{n}} $ has a standard normal
  distribution.
\end{remark}



\begin{example}[Example 7.2 in textbook]
  \label{ex:example-7.2-textbook-bottling}
  A bottling machine discharages an average of $\mu$ ounces per bottle. It has
  been observed that the amount of liquid discharged is normally distributed
  with standard deviation $\sigma=1$ per ounce. A sample of $n=9$ filled
  bottles is randomly selected, and is measured.

  \Fl{Question:} Find the probability that the sample mean will be within
  $.3$ ounces of the true mean $\mu$.


  \Fl{Solution:} Let $Y_{1},\ldots,Y_{9}$ be the ounces in each of the
  bottles. Then $Y_{i}\sim \mathcal{N}(\mu,1)$ for $i\in [9]$. By
  \cref{thm:mean-and-variance-of-normal-sample-mean}, $\overline{Y}\sim \mathcal{N}(\mu,
  \sigma^{2}/n)$, and $\sigma^{2}/n=1/9$. We want to find
  \begin{align}\label{eq:4}
    \P\left[\left| \overline{Y}-\mu \right| \leq .3\right] 
    &= \P\left[-.3 \leq \overline{Y}-\mu \leq .3\right] \nonumber\\
    &= \P\left[- \frac{.3}{\sigma/\sqrt{n}}\leq \frac{\overline{Y}-\mu}{\sigma/\sqrt{n}} \leq \frac{.3}{\sigma/\sqrt{n}}\right].
  \end{align}
  By \cref{rmk:sample-mean-standardized},
  $Z=\frac{\overline{Y}-\mu}{\sigma/\sqrt{n}}\sim \mathcal{N}(0,1)$, and
  plugging in $\sigma=1$ and $n=9$ and simplifying gives
  \begin{align*}
    \P\left[\left| \overline{Y}-\mu \right| \leq .3\right] 
    &= \P\left[-.9 \leq Z \leq .9\right]\\
    &= 1- 2 \P\left[Z>.9 \right]\\
    &=.6318.
  \end{align*}
\end{example}


\begin{example}[Example 7.3 in textbook]
  How many observations need to be included in the sample to ensure that
  $\overline{Y}$ is within $.3$ ounces of $\mu$ with probability $.95$?

  Now we want
  \begin{equation*}
    \P\left[\left| \overline{Y} -\mu \right| \leq .3\right] = .95.
  \end{equation*}
  By \cref{eq:4}, 
  \begin{equation*}
    \P\left[\left| \overline{Y} -\mu \right| \leq .3\right] = \P\left[- \frac{.3}{\sigma/\sqrt{n}}\leq \frac{\overline{Y}-\mu}{\sigma/\sqrt{n}} \leq \frac{.3}{\sigma/\sqrt{n}}\right].
  \end{equation*}
  Recalling that $\sigma=1$ and
  $Z=\frac{\overline{Y}-\mu}{\sigma/\sqrt{n}}\sim \mathcal{N}(0,1)$, we have
  \begin{equation*}
    \P\left[\left| \overline{Y}-\mu \right| \leq .3 \right] = \P\left[.3 \sqrt{n}\leq Z \leq .3 \sqrt{n}\right].
  \end{equation*}
  So we need to find $n$ such that $\P\left[.3 \sqrt{n}\leq Z \leq .3
    \sqrt{n}\right]=.95$.

  It is a well-known fact that
  \begin{equation*}
    \P\left[-1.96 \leq Z \leq 1.96\right] = .95.
  \end{equation*}
  So we need $.3 \sqrt{n}=1.96$, or equivalently, $n= \left( \frac{1.96}{.3}
  \right)^{2}= 42.68$.

  So we need at least $43$ samples, since $42$ is not quite enough.
\end{example}

\newpage
\section{2026-02-02 | Week 04 | Lecture 09}

\textit{This lecture is based on Section 7.2 in the textbook. I will be
  skipping the part about F statistics for now.}

\subsection{``Approximate'' standardization}

In \cref{ex:example-7.2-textbook-bottling}, we had a random sample
$Y_{1},\ldots,Y_{n}$ of normally distributed random variables, and we computed
\begin{equation*}
  \P\left[\left| \overline{Y}-\mu \right| \leq .3\right]
\end{equation*}
by rewriting the even $\left[ \left| \overline{Y}-\mu \right| \leq .3\right]$
using the standardization trick:
\begin{align*}
  \left[ \left| \overline{Y}-\mu \right| \leq .3\right] 
  &= \left[ -.3 \leq \overline{Y} -\mu \leq .3\right]\\
  &= \left[ \frac{-.3}{\sigma/\sqrt{n}} \leq \frac{\overline{Y} -\mu}{\sigma/\sqrt{n}} \leq \frac{.3}{\sigma/\sqrt{n}}\right]\\
  &= \left[ \frac{-.3}{\sigma/\sqrt{n}} \leq Z \leq \frac{.3}{\sigma/\sqrt{n}}\right]
\end{align*}
where $Z\sim \mathcal{N}(0,1)$. In other words, we used standardization (see
\cref{rmk:standardization}) to rewrite the probability into something we could
compute (or look up).

The problem with this approach is that the example is contrived and
unrealistic because, in the problem statement, we assumed that we knew the
true standard deviation to be $\sigma=1$. In practice, this sort of thing is
almost never known ahead of time, but instead must be estimated from the data.

A natural estimator of $\sigma$ is $S$, the square root of the sample
variance. This is because $S^{2} \to \sigma^{2}$ as $n\to \infty$ by the law
of large numbers, so $S$ should be close to $\sigma$ when $n$ is large.

In that case, we can \textbf{``approximately'' standardize}, by writing
\begin{equation*}
  \P\left[\left| \overline{Y}-\mu \right| \leq .3\right]
  = \left[ \frac{-.3}{S/\sqrt{n}} \leq \underbrace{\frac{\overline{Y}
        -\mu}{S/\sqrt{n}}}_{\text{call it }T} \leq \frac{.3}{S/\sqrt{n}}\right]
\end{equation*}

The random variable $T$ is not normally distributed, but it should be pretty
close to $\mathcal{N}(0,1)$ when $n$ is large. This is because
$S\approx \sigma$ when $n$ is large, so
\begin{align*}
  T 
  &= \left( \frac{\overline{Y}-\mu}{S/\sqrt{n}} \right) \\
  &\approx \left( \frac{\overline{Y}-\mu}{S/\sqrt{n}} \right)\\
  &= Z
\end{align*}
where $Z\sim \mathcal{N}(0,1)$.

\Fl{Question:} What is the distribution of $T$? Can we quantify ``how close''
it is to $Z$?

\Fl{Short answer:} Assuming a sample size of $n>2$, $T$ has a distribution
called a $t$-distribution, which is well-known and widely used. It has
expectation $\E\left[T \right]=0$ and variance $\Var(T) = \frac{\nu}{\nu-2}$,
where $\nu=n-1$ is called the ``degrees of freeom''. It
has a probability density function which is symmetric and bell-shaped, like
the normal distribution, but the shape is a little more spread out than the
standard normal:
\begin{center}
  \includegraphics[scale=.3]{images/t-vs-Z}
\end{center}
The reason for this greater spread is that $\Var(T)>1$ for all $n>2$, whereas
$\Var(Z)=1$.

\Fl{Long answer:} see the next two subsections.

\subsection{The chi-squared distribution}

\begin{definition}[Chi-Squared Distribution]
  \label{def:chi-squared-distribution}
  Let $k$ be a positive integer. A nonnegative random variable $X$ is said to have
  \demph{chi-squared distribution with parameter $d$} if it has pdf
  \begin{equation*}
    f(x) 
    = \left\{ 
      \begin{array}{l@{\quad:\quad}l} \frac{1}{2^{d/2}\Gamma(d/2)}x^{\frac{d}{2}-1}e^{-x/2} & x \geq 0\\ 0& x<0\end{array}\right.
  \end{equation*}
  In other words, if $X$ is a Gamma distributed random variable with
  parameters $\alpha=d/2$ and $\beta=2$. The parameter $d$ is usually called
  the \demph{degrees of freedom} of $X$.
\end{definition}

From left-to-right, here's what this looks like for $d=2$, $d=4$, and $d=12$:
\begin{center}
  \includegraphics[scale=0.15]{images/chi-squared-d2}
  \includegraphics[scale=0.15]{images/chi-squared-d4}
  \includegraphics[scale=0.15]{images/chi-squared-d12}
\end{center}

Using properties of gamma distribution, we have:
\begin{proposition}
  A $\chi^{2}$ random variable $X$ with $d$ degrees of freedom has
  \begin{equation*}
    \E\left[X \right] = \alpha\beta = d \quad \text{and} \quad \Var(X) = \alpha\beta^{2} = 2d
  \end{equation*}
\end{proposition}

Chi-squared random variables are important in statistics because they arise as
squared normal random variables:

\begin{theorem}[Chi-squared is sum of squared normals]
  \label{thm:chi-squared-sum-squared-normals}
  Let $n \geq 1$. If $Z_{1},\ldots,Z_{n}$ are independent standard normal
  random variables, then the random variable $V=Z_{1}^{2}+\ldots+Z_{n}^{2}$
  has chi-squared distribution with $n$ degrees of freedom $(\mathrm{df}=n)$. 
\end{theorem}
\begin{proof}
  Proved using moment generating functions. (See Example 6.11 in the textbook).
\end{proof}

\begin{remark}
  Taking $n=1$, \cref{thm:chi-squared-sum-squared-normals} implies that $Z \sim
  \mathcal{N}(0,1)$ implies $Z^{2}$ is chi-squared with $1$ degree of freedom.
\end{remark}

\begin{corollary}[Theorem 7.2 in textbook]
  \label{claim:theorem-7.2-textbook}
  If $Y_{1},\ldots,Y_{n}$ is a sample of $\mathcal{N}(\mu,\sigma)$ random
  variables, then
  \begin{equation*}
    \sum_{i=1}^{n} \left( \frac{Y_{i}-\mu}{\sigma} \right)^{2} 
  \end{equation*}
  is a chi-squared random variable with $\mathrm{df}=n$. 
\end{corollary}

\begin{remark}
  Usually, we use a table or a computer to compute probabilites with
  chi-squared random variables. But the fact that we usually can't do exact
  computations by hand doesn't make chi-squared random variables any less
  important to statistics, as they show up in samples variances, linear
  regression, distance estimates (pythagorean theorem has squares), etc.
\end{remark}

The main example we will need is the following
\begin{theorem}[Theorem 7.3 in textbook]
  \label{thm:theorem-7.3-textbook}
  Let $Y_{1},\ldots,Y_{n}$ be a random sample from
  $\mathcal{N}(\mu,\sigma^{2})$. Then the random variable
  \begin{equation*}
    \frac{(n-1)S^{2}}{\sigma^{2}}
  \end{equation*}
  has a chi-squared distribution with $n-1$ degrees of freedom. Also, the
  random variables $\overline{Y}$ and $S^{2}$ are independent.
\end{theorem}

See textbook for outline of the proof (for the case when $n=2$). The
independence of $\overline{Y}$ and $S^{2}$ is rather technical to prove, but
simply says that the knowledge of the center of location of a normal random
variable does not contribute to knowledge of its variability.



\subsection{Definition of the Student t-distribution}

\begin{definition}[Student's t-distribution]
  \label{def:students-t-distribution}
  Let $Z\sim \mathcal{N}(0,1)$ and let $\chi^{2}_{\nu}$ be an independent
  chi-squared distributed random variable with $\textrm{df}=\nu$. Then
  \begin{equation*}
    T = \frac{Z}{\sqrt{\chi^{2}_{\nu}/\nu}}
  \end{equation*}
  is said to have a \demph{t-distribution}\footnote{This is also called
    ``Student's t distribution''. It was discovered by W.~S.~Gosset, an
    employee of the Guinness Brewery in Dublin, who wrote under the pseudonym
    ``Student.''} with $\nu$ degrees of freedom.
\end{definition}



\begin{remark}
  \begin{itemize}
    \item []
    \item To show that a random variable follows a $T$ distribution, we must
    show that it can be written as a ratio of a standard normal random
    variable to the square root of an independent chi-squared random variable
    divided by its degrees of freedom.
    \item There are infinitely many t-distributions, one for every positive
    integral $\nu$.
    \item As $\nu$ increases, the bell-curve distribution of $T$ becomse less
    spread out, and approaches the standard normal curve as $\nu \to\infty$.
  \end{itemize}
\end{remark}


The following theorem illustrates an important example of the
$t$-distribution, which arises when we ``approximately standardize'' a sample
mean using $S$ rather thatn $\sigma$.


\begin{theorem}
  Let $Y_{1},\ldots,Y_{n}$ be a random sample from
  $\mathcal{N}(\mu,\sigma^{2})$. The random variable
  \begin{equation*}
    \frac{\overline{Y}-\mu}{S/\sqrt{n}}
  \end{equation*}
  follows a $T$ distribution with $n-1$ degrees of freedom.
\end{theorem}
\begin{proof}
  Observe that
  \begin{itemize}
    \item $Z := \frac{\overline{Y}-\mu}{\sigma/\sqrt{n}}$ is a standard normal by
    \cref{rmk:standardization} and \cref{thm:mean-and-variance-of-normal-sample-mean}.
    \item $\chi^{2}_{n-1} := (n-1)S^{2}/\sigma^{2}$ has a $\chi^{2}$
    distribution with $n-1$ degrees of freedom by
    \cref{thm:theorem-7.3-textbook}.
    \item Since $\overline{Y}\indep S^{2}$ by \cref{thm:theorem-7.3-textbook},
    it follows that $Z\indep \chi^{2}_{n-1}$ as well.
  \end{itemize}
  Hence
  \begin{align*}
    \frac{Y-\mu}{S/\sqrt{n}}
    &= \frac{\frac{\overline{Y}-\mu}{\sigma/\sqrt{n}}}{\sqrt{\left[ (n-1)S^{2}/\sigma^{2} \right]/(n-1) }} \\
    &:= \frac{Z}{\sqrt{\chi^{2}_{n-1}/(n-1)}},
  \end{align*}
  And the right hand satisfies the definition of a t-distribution with
  $\nu=n-1$ degrees of freedom.
\end{proof}


\begin{example}[Example 7.6 in the textbook]
  \label{ex:example-7.6}
  The tensile strengh of a type of wire used in the detonator of a
  plutonium-based nuclear weapon is normally distributed with unknown mean
  $\mu\in \R$ and variance $\sigma^{2}>0$.

  Six pieces of wire are selected from a roll. Let
  \begin{equation*}
    Y_{i} := \text{the tensile strength for portion }i, \quad (i\in [6]).
  \end{equation*}
  We can estimate $\mu$ and $\sigma^{2}$ with $\overline{Y}$ and $S^{2}$.
  Because $\sigma^{2}_{\overline{Y}}=\sigma^{2}/n$, it follows that
  $\sigma^{2}_{\overline{Y}}$ can be estimated by $S^{2}/n$.

  \Fl{Question:} Find the probability that $\overline{Y}$ is within
  $2S/\sqrt{n}$ of $\mu$.

  \begin{align*}
    \P\left[ \left| \overline{Y}-\mu \right| \leq \frac{2S}{\sqrt{n}}\right]
    &= \P\left[-\frac{2S}{\sqrt{n}}\leq  \overline{Y}-\mu \leq \frac{2S}{\sqrt{n}}\right]\\
    &= \P\left[-2 \leq  \left( \frac{\overline{Y}-\mu}{S/\sqrt{n}} \right) \leq 2\right]\\
    &= \P\left[-2 \leq T \leq  2\right],
  \end{align*}
  where $T$ has a t-distribuion with $\mathrm{df}=5$ (i.e., one less than the number
  of samples).

  A computer tells us that when $\mathrm{df}=5$,
  \begin{equation*}
    \P\left[-2.015 \leq T \leq  2.015\right] = .9
  \end{equation*}
  Hence, $\P\left[ \left| \overline{Y}-\mu \right| \leq
    \frac{2S}{\sqrt{n}}\right]$ is slightly less than $.9$.  
\end{example}

\section{2026-02-04 | Week 04 | Lecture 10}

\textit{Section 7.3. The central limit theorem :)}

We've seen that if $Y_{1},\ldots,Y_{n}$ is a random sample of normal random
variables, then $\overline{Y}$ is normally distributed.

What if the population is not normally distributed?

\Fl{Main idea of the central limit theorem:} If  $X_{1},\ldots,X_{n}$ is a
random sample from (almost) any distribution, then $\overline{X}$ will be
approximately normal when $n$ is large.



\begin{theorem}[Central Limit Theorem]
  \label{thm:central-limit-theorem}
  Let $X_{1},\ldots,X_{n}$ be independent identically distributed random
  variables with $\E\left[X_{i} \right]=\mu$ and $\Var(X_{i}) =
  \sigma^{2}<\infty$. Let
  \begin{equation*}
    U_{n} = \frac{\overline{X}-\mu}{\sigma \sqrt{n}}.
  \end{equation*}
  Then
  \begin{equation*}
    U_{n} \to \mathcal{N}(0,1) \text{ as } n \to \infty,
  \end{equation*}
  in the sense that
  \begin{equation*}
    \lim_{n\to \infty}\P\left[U_{n}\leq u\right] =
    \int_{-\infty}^{u}\frac{1}{\sqrt{2\pi}}e^{-t^{2}/2}dt \text{ for all $u\in \R$.}
  \end{equation*}
\end{theorem}

In other words, $\overline{X}$ is \textit{asymptotically normally distributed
  with mean $\mu$ and variance $\sigma^{2}/n$.}, meaning that for any
$-\infty \leq a<b \leq \infty$,
\begin{equation*}
  \P\left[a \leq \overline{X} \leq b\right] \approx \P\left[a \leq W \leq b\right]
\end{equation*}
when $n$ is large, and $W\sim \mathcal{N}(\mu, \sigma^{2}/n)$.


\end{document}